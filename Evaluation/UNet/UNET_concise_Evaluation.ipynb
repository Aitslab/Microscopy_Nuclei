{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19vwajeUmEPx"
   },
   "source": [
    "# Evaluation of U-Net \n",
    "\n",
    "### Final Edit by Salma and Klara\n",
    "\n",
    "27th Of Feb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFGTMPcx7_170702000001_B23f07d0.png\r\n",
      "MFGTMPcx7_170702000001_F11f10d0.png\r\n",
      "MFGTMPcx7_170702090001_A08f09d0.png\r\n",
      "MFGTMPcx7_170702090001_A20f02d0.png\r\n",
      "MFGTMPcx7_170702090001_G03f02d0.png\r\n",
      "MFGTMPcx7_170702090001_K22f14d0.png\r\n",
      "MFGTMPcx7_170702090001_P01f02d0.png\r\n",
      "MFGTMPcx7_170731090001_A01f04d0.png\r\n",
      "MFGTMPcx7_170731090001_B05f12d0.png\r\n",
      "MFGTMPcx7_170802000001_I10f05d0.png\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../data/4_filelists/VALIDATION.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFGTMPcx7_170702000001_G14f03d0.png\r\n",
      "MFGTMPcx7_170702090001_B22f15d0.png\r\n",
      "MFGTMPcx7_170702090001_C08f14d0.png\r\n",
      "MFGTMPcx7_170702090001_H04f01d0.png\r\n",
      "MFGTMPcx7_170702090001_P07f14d0.png\r\n",
      "MFGTMPcx7_170731090001_B14f09d0.png\r\n",
      "MFGTMPcx7_170731090001_D11f13d0.png\r\n",
      "MFGTMPcx7_170731090001_I12f05d0.png\r\n",
      "MFGTMPcx7_170801050001_G02f01d0.png\r\n",
      "MFGTMPcx7_170803210001_J12f29d0.png\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../data/4_filelists/TEST.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qSa6leN9mEP2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "import skimage.io\n",
    "import skimage.morphology\n",
    "import skimage.segmentation\n",
    "\n",
    "from config import config_vars\n",
    "import utils\n",
    "\n",
    "import  utils.metrics\n",
    "#! pip install pandas==0.23.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzVPXA30kvwk"
   },
   "source": [
    "### Evalution metrics and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ZBm9b-nbmwsr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def intersection_over_union(ground_truth, prediction):\n",
    "    \n",
    "    # Count objects\n",
    "    true_objects = len(np.unique(ground_truth))\n",
    "    pred_objects = len(np.unique(prediction))  \n",
    "    \n",
    "    # Compute intersection\n",
    "    h = np.histogram2d(ground_truth.flatten(), prediction.flatten(), bins=(true_objects,pred_objects))\n",
    "    intersection = h[0]\n",
    "    \n",
    "    # Area of objects\n",
    "    area_true = np.histogram(ground_truth, bins=true_objects)[0]\n",
    "    area_pred = np.histogram(prediction, bins=pred_objects)[0]\n",
    "    \n",
    "    # Calculate union\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "    union = area_true + area_pred - intersection\n",
    "    \n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    \n",
    "    # Compute Intersection over Union\n",
    "    union[union == 0] = 1e-9\n",
    "    IOU = intersection/union\n",
    "    \n",
    "    return IOU\n",
    "    \n",
    "\n",
    "\n",
    "def measures_at(threshold, IOU):\n",
    "    \n",
    "    matches = IOU > threshold\n",
    "    \n",
    "    true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "   \n",
    "    false_negatives = np.sum(matches, axis=1) == 0  # Missed objects\n",
    "    \n",
    "    \n",
    "    FP_matches = IOU > 0.5\n",
    "    false_positives = np.sum(FP_matches, axis=0) == 0  # Extra objects\n",
    "    \n",
    "    assert np.all(np.less_equal(true_positives, 1))\n",
    "    assert np.all(np.less_equal(false_positives, 1))\n",
    "    assert np.all(np.less_equal(false_negatives, 1))\n",
    "    \n",
    "    TP, FP, FN = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "    \n",
    "    f1 = 2*TP / (2*TP + FP + FN + 1e-9)\n",
    "    \n",
    "    return f1, TP, FP, FN\n",
    "\n",
    "# Compute Average Precision for all IoU thresholds\n",
    "\n",
    "def compute_af1_results(ground_truth, prediction, results, image_name):\n",
    "   \n",
    "    # Compute IoU\n",
    "    IOU = intersection_over_union(ground_truth, prediction)\n",
    "    if IOU.shape[0] > 0:\n",
    "        jaccard = np.max(IOU, axis=0).mean()\n",
    "    else:\n",
    "        jaccard = 0.0\n",
    "    \n",
    "    # Calculate F1 score at all thresholds\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        f1, tp, fp, fn = measures_at(t, IOU)\n",
    "        res = {\"Image\": image_name, \"Threshold\": t, \"F1\": f1, \"Jaccard\": jaccard, \"TP\": tp, \"FP\": fp, \"FN\": fn}\n",
    "        row = len(results)\n",
    "        results.loc[row] = res\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Count number of False Negatives at 0.9 IoU\n",
    "\n",
    "def get_false_negatives(ground_truth, prediction, results, image_name, threshold=0.9):\n",
    "    \n",
    "    # Compute IoU\n",
    "    IOU = intersection_over_union(ground_truth, prediction)\n",
    "    \n",
    "    true_objects = len(np.unique(ground_truth))\n",
    "    if true_objects <= 1:\n",
    "        return results\n",
    "        \n",
    "    area_true = np.histogram(ground_truth, bins=true_objects)[0][1:]\n",
    "    true_objects -= 1\n",
    "    \n",
    "    # Identify False Negatives\n",
    "    matches = IOU > threshold\n",
    "    false_negatives = np.sum(matches, axis=1) == 0  # Missed objects\n",
    "\n",
    "    data = np.asarray([ \n",
    "        area_true.copy(), \n",
    "        np.array(false_negatives, dtype=np.int32)\n",
    "    ])\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame(data=data.T, columns=[\"Area\", \"False_Negative\"])])\n",
    "        \n",
    "    return results\n",
    "\n",
    "### WE changed the threshold of FP to 0.5\n",
    "### This might change so many things\n",
    "### So, the sum of TP and FP is not predicted objects since some objects are not FP and not also TP.\n",
    "### FP is calculated from FN (0.9)\n",
    "\n",
    "def get_false_positives(ground_truth, prediction, results, image_name, threshold=0.5):\n",
    "    \n",
    "    # Compute IoU\n",
    "    IOU = intersection_over_union(ground_truth, prediction)\n",
    "    \n",
    "    pred_objects = len(np.unique(prediction))\n",
    "    if pred_objects <= 1:\n",
    "        return results\n",
    "        \n",
    "    area_pred = np.histogram(prediction, bins = pred_objects)[0][1:]\n",
    "    pred_objects -= 1\n",
    "    # Identify False Positives\n",
    "    matches = IOU > threshold\n",
    "    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "    \n",
    "    data = np.asarray([ \n",
    "        area_pred.copy(), \n",
    "        np.array(false_positives, dtype=np.int32)\n",
    "    ])\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame(data=data.T, columns=[\"Area\", \"False_Positive\"])])\n",
    "        \n",
    "    return results\n",
    "\n",
    "### I have added this here to calculate the area of all gold objects\n",
    "\n",
    "def get_gold_objects(ground_truth, prediction, results, image_name):\n",
    "    \n",
    "    \n",
    "    true_objects = len(np.unique(ground_truth))\n",
    "    if true_objects <= 1:\n",
    "        return results\n",
    "    \n",
    "    area_true = np.histogram(ground_truth, bins=true_objects)[0][1:]\n",
    "    \n",
    "    true_objects -= 1\n",
    "    \n",
    "    \n",
    "    data = np.asarray([ \n",
    "        np.arange(true_objects),\n",
    "        area_true.copy()])\n",
    "   \n",
    "    results = pd.concat([results, pd.DataFrame(data=data.T, columns=[\"Gold_Object\", \"Area\"])])\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "## This is for calculating the area of all predicted objects\n",
    "def get_predicted_objects(ground_truth, prediction, results, image_name):\n",
    "    \n",
    "    \n",
    "    pred_objects = len(np.unique(prediction))\n",
    "    if pred_objects <= 1:\n",
    "        return results\n",
    "    \n",
    "    area_pred = np.histogram(prediction, bins=pred_objects)[0][1:]\n",
    "    \n",
    "    pred_objects -= 1\n",
    "    \n",
    "    \n",
    "    data = np.asarray([ \n",
    "        np.arange(pred_objects),\n",
    "        area_pred.copy()])\n",
    "   \n",
    "    results = pd.concat([results, pd.DataFrame(data=data.T, columns=[\"Pred_Object\", \"Area\"])])\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Count the number of splits and merges\n",
    "def get_splits_and_merges(ground_truth, prediction, results, image_name):\n",
    "\n",
    "    # Compute IoU\n",
    "    IOU = intersection_over_union(ground_truth, prediction)\n",
    "    \n",
    "    matches = IOU > 0.1\n",
    "    merges = np.sum(matches, axis=0) > 1\n",
    "    splits = np.sum(matches, axis=1) > 1\n",
    "    r = {\"Image_Name\":image_name, \"Merges\":np.sum(merges), \"Splits\":np.sum(splits)}\n",
    "    results.loc[len(results)+1] = r\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-DIQl9FmEP7"
   },
   "source": [
    "### Auxiliary visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "PLiUUD-9mEP7"
   },
   "outputs": [],
   "source": [
    "# Display prediction along with segmentation to visualize errors\n",
    "\n",
    "def show(ground_truth, prediction, threshold=0.9, image_name=\"N\"):\n",
    "    \n",
    "    # Compute Intersection over Union\n",
    "    IOU = intersection_over_union(ground_truth, prediction)\n",
    "    \n",
    "    # Create diff map\n",
    "    diff = np.zeros(ground_truth.shape + (3,))\n",
    "    A = ground_truth.copy()\n",
    "    B = prediction.copy()\n",
    "    A[A > 0] = 1\n",
    "    B[B > 0] = 1\n",
    "    D = A - B\n",
    "    #diff[D > 0,:2] = 1\n",
    "    #diff[D < 0,1:] = 1\n",
    "    \n",
    "    # Object-level errors\n",
    "    C = IOU.copy()\n",
    "    C[C>=threshold] = 1\n",
    "    C[C<threshold] = 0\n",
    "    extra = np.where(np.sum(C,axis=0) == 0)[0]\n",
    "    \n",
    "    \n",
    "    ### We changed the threshold of FP to 0.5 even here in show function\n",
    "    FP_threshold = 0.5\n",
    "    P = IOU.copy()\n",
    "    P[P>=FP_threshold] = 1\n",
    "    P[P<FP_threshold] = 0\n",
    "    missed = np.where(np.sum(P,axis=1) == 0)[0]\n",
    "    \n",
    "    print()\n",
    "    print('missed: '+str(missed))\n",
    "    print()\n",
    "    print('extra: '+str(extra))\n",
    "\n",
    "    for m in missed:\n",
    "        diff[ground_truth == m+1, 0] = 1\n",
    "    for e in extra:\n",
    "        diff[prediction == e+1, 2] = 1\n",
    "    \n",
    "    # Display figures\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(25,10))\n",
    "    ax[0].imshow(ground_truth)\n",
    "    ax[0].set_title(\"True objects:\"+str(len(np.unique(ground_truth))-1))# correction corr script: -1\n",
    "    ax[1].imshow(diff)\n",
    "    ax[1].set_title(\"Segmentation errors (missed):\"+str(len(missed)))\n",
    "    ax[2].imshow(prediction)\n",
    "    ax[2].set_title(\"Predicted objects:\"+str(len(np.unique(prediction))-1))# correction corr script: -1\n",
    "    #ax[3].imshow(IOU)\n",
    "    #ax[3].set_title(image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG2ac32vmEP8"
   },
   "source": [
    "# Run the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqJi2ziblx51"
   },
   "source": [
    "## This part is for evaluating UNet models  3, 12, and 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pyuJjHkQ78u",
    "outputId": "3fb8459e-ed8e-4011-d61c-914c31c50aba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'home_folder': '/proj/berzelius-2021-21/users/klara/Segmentation/data/',\n",
       " 'model_file': '/proj/berzelius-2021-21/users/klara/Segmentation/data/experiments/Model_3_Malou/model.hdf5',\n",
       " 'input_dimensions': 1,\n",
       " 'root_directory': '/proj/berzelius-2021-21/users/klara/Segmentation/data/',\n",
       " 'learning_rate': 0.0001,\n",
       " 'epochs': 15,\n",
       " 'cell_min_size': 10,\n",
       " 'max_training_images': 130,\n",
       " 'steps_per_epoch': 500,\n",
       " 'pixel_depth': 8,\n",
       " 'batch_size': 1,\n",
       " 'val_batch_size': 1,\n",
       " 'rescale_labels': True,\n",
       " 'crop_size': 256,\n",
       " 'boundary_boost_factor': 1,\n",
       " 'object_dilation': 3,\n",
       " 'raw_annotations_dir': '../data/1_raw_annotations/',\n",
       " 'normalized_images_dir': '/proj/berzelius-2021-21/users/klara/Segmentation/data/norm_images/',\n",
       " 'boundary_labels_dir': '/proj/berzelius-2021-21/users/klara/Segmentation/data/boundary_labels/',\n",
       " 'path_files_training': '../data/4_filelists/training.txt',\n",
       " 'path_files_validation': '../data/4_filelists/VALIDATION.txt',\n",
       " 'path_files_test': '../data/4_filelists/TEST.txt',\n",
       " 'experiment_dir': '/proj/berzelius-2021-21/users/klara/Segmentation/data/experiments/Model_3_Malou/out/',\n",
       " 'probmap_out_dir': '/proj/berzelius-2021-21/users/klara/Segmentation/data/experiments/Model_3_Malou/out/prob/',\n",
       " 'labels_out_dir': '/proj/berzelius-2021-21/users/klara/Segmentation/data/experiments/Model_3_Malou/out/segm/',\n",
       " 'csv_log_file': '/proj/berzelius-2021-21/users/klara/Segmentation/data/experiments/Model_3_Malou/log.csv',\n",
       " 'best_model': '/proj/berzelius-2021-21/users/klara/Segmentation/data/experiments/Model_3_Malou/_'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils.dirtools\n",
    "\n",
    "# Partition of the data to make predictions (test or validation)\n",
    "config_vars['path_files_training'] = '../data/4_filelists/training.txt'\n",
    "config_vars['path_files_validation'] ='../data/4_filelists/VALIDATION.txt'\n",
    "config_vars['path_files_test'] = '../data/4_filelists/TEST.txt'\n",
    "\n",
    "partition = \"validation\"\n",
    "\n",
    "experiment_name = 'Model_3_Malou'\n",
    "\n",
    "config_vars = utils.dirtools.setup_experiment(config_vars, experiment_name)\n",
    "\n",
    "data_partitions = utils.dirtools.read_data_partitions(config_vars)\n",
    "\n",
    "config_vars\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MFGTMPcx7_170702000001_G14f03d0.png',\n",
       " 'MFGTMPcx7_170702090001_B22f15d0.png',\n",
       " 'MFGTMPcx7_170702090001_C08f14d0.png',\n",
       " 'MFGTMPcx7_170702090001_H04f01d0.png',\n",
       " 'MFGTMPcx7_170702090001_P07f14d0.png',\n",
       " 'MFGTMPcx7_170731090001_B14f09d0.png',\n",
       " 'MFGTMPcx7_170731090001_D11f13d0.png',\n",
       " 'MFGTMPcx7_170731090001_I12f05d0.png',\n",
       " 'MFGTMPcx7_170801050001_G02f01d0.png',\n",
       " 'MFGTMPcx7_170803210001_J12f29d0.png']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "all_images = data_partitions['test']\n",
    "all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfReemqr2_GE"
   },
   "source": [
    "## Run evaluation (3, 12, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3NL81E2WmEP9",
    "outputId": "cfcf326a-f866-41c8-e5f6-08edcf28a726",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFGTMPcx7_170702000001_G14f03d0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/berzelius-2021-21/users/Salma-files/envs/UNET_TF_1/lib/python3.6/site-packages/ipykernel_launcher.py:101: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/proj/berzelius-2021-21/users/Salma-files/envs/UNET_TF_1/lib/python3.6/site-packages/ipykernel_launcher.py:130: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFGTMPcx7_170702090001_B22f15d0.png\n",
      "MFGTMPcx7_170702090001_C08f14d0.png\n",
      "MFGTMPcx7_170702090001_H04f01d0.png\n",
      "MFGTMPcx7_170702090001_P07f14d0.png\n",
      "MFGTMPcx7_170731090001_B14f09d0.png\n",
      "MFGTMPcx7_170731090001_D11f13d0.png\n",
      "MFGTMPcx7_170731090001_I12f05d0.png\n",
      "MFGTMPcx7_170801050001_G02f01d0.png\n",
      "MFGTMPcx7_170803210001_J12f29d0.png\n"
     ]
    }
   ],
   "source": [
    "#all_images = data_partitions[partition]\n",
    "\n",
    "config_vars[\"raw_annotations_dir\"] = '../data/1_raw_annotations/'\n",
    "config_vars[\"labels_out_dir\"]      =\\\n",
    "'/proj/berzelius-2021-21/users/klara/Segmentation/data/experiments/Model_3_Malou/out_14/segm/'\n",
    "all_images = data_partitions['test']\n",
    "\n",
    "config_vars[\"object_dilation\"] = 3\n",
    "\n",
    "from skimage.color import rgb2gray,rgb2lab\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Image\", \"Threshold\", \"F1\", \"Jaccard\", \"TP\", \"FP\", \"FN\"])\n",
    "\n",
    "false_negatives = pd.DataFrame(columns=[\"False_Negative\", \"Area\"])\n",
    "false_positives = pd.DataFrame(columns=[\"False_Positive\", \"Area\"])\n",
    "gold_obj = pd.DataFrame(columns=[\"Gold_Object\", \"Area\"])  # for counting the area of gold objects\n",
    "pred_obj = pd.DataFrame(columns=[\"Pred_Object\", \"Area\"])  # for counting area of predicted objects\n",
    "      \n",
    "        \n",
    "        \n",
    "splits_merges = pd.DataFrame(columns=[\"Image_Name\", \"Merges\", \"Splits\"])\n",
    "\n",
    "\n",
    "\n",
    "for image_name in all_images:\n",
    "    print(image_name)\n",
    "    # Load ground truth data\n",
    "    img_filename = os.path.join(config_vars[\"raw_annotations_dir\"], image_name)\n",
    "    ground_truth = skimage.io.imread(img_filename)\n",
    "    #ground_truth = ground_truth.squeeze()\n",
    "    if len(ground_truth.shape) == 3:\n",
    "        ground_truth = rgb2lab(ground_truth)\n",
    "        ground_truth = ground_truth[:,:,0]\n",
    "    \n",
    "    ground_truth = skimage.morphology.label(ground_truth)\n",
    "    \n",
    "    # Transform to label matrix\n",
    "    #ground_truth = skimage.morphology.label(ground_truth)\n",
    "    \n",
    "    # Load predictions\n",
    "    pred_filename = os.path.join(config_vars[\"labels_out_dir\"], image_name)\n",
    "    prediction = skimage.io.imread(pred_filename)\n",
    "    \n",
    "    \n",
    "    if len(prediction.shape) == 3:\n",
    "        prediction = rgb2lab(prediction)\n",
    "        prediction = prediction[:,:,0]\n",
    "    \n",
    "    if config_vars[\"object_dilation\"] > 0:\n",
    "            struct = skimage.morphology.square(config_vars[\"object_dilation\"])\n",
    "            prediction = skimage.morphology.dilation(prediction, struct)\n",
    "    elif config_vars[\"object_dilation\"] < 0:\n",
    "            struct = skimage.morphology.square(-config_vars[\"object_dilation\"])\n",
    "            prediction = skimage.morphology.erosion(prediction, struct)\n",
    "\n",
    "    ####################################################################################    \n",
    "    #### Testing prediction with no small objects on annot and prediction #####\n",
    "    ground_truth = skimage.morphology.remove_small_objects(ground_truth, min_size=10)##min is 25 \n",
    "    prediction = skimage.morphology.remove_small_objects(prediction, min_size=10)\n",
    "    #####################################################################################\n",
    "    \n",
    "    # Relabel objects (cut margin of 30 pixels to make a fair comparison with DeepCell)\n",
    "    ground_truth = skimage.segmentation.relabel_sequential(ground_truth)[0] #[30:-30,30:-30])[0]\n",
    "    prediction = skimage.segmentation.relabel_sequential(prediction)[0] #[30:-30,30:-30])[0]\n",
    "  \n",
    "    # Compute evaluation metrics\n",
    "    results = compute_af1_results(\n",
    "        ground_truth, \n",
    "        prediction, \n",
    "        results, \n",
    "        image_name\n",
    "    )\n",
    "    \n",
    "    false_negatives = get_false_negatives(\n",
    "        ground_truth, \n",
    "        prediction, \n",
    "        false_negatives, \n",
    "        image_name\n",
    "    )\n",
    "    \n",
    "    false_positives = get_false_positives(\n",
    "        ground_truth, \n",
    "        prediction, \n",
    "        false_positives, \n",
    "        image_name\n",
    "    )\n",
    "    \n",
    "    splits_merges = get_splits_and_merges(\n",
    "        ground_truth, \n",
    "        prediction, \n",
    "        splits_merges, \n",
    "        image_name\n",
    "    )\n",
    "    \n",
    "    gold_obj = get_gold_objects(\n",
    "          ground_truth, \n",
    "          prediction, \n",
    "          gold_obj, \n",
    "          image_name\n",
    "      )\n",
    "    pred_obj = get_predicted_objects(\n",
    "          ground_truth, \n",
    "          prediction, \n",
    "          pred_obj, \n",
    "          image_name\n",
    "      )\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>False_Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1781</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1774</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1765</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1357</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>469</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1478</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1450</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1682</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1596</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1581</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1952</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2257</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1433</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1837</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1508</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1558</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1629</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1456</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1401</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area False_Negative\n",
       "0   1781              0\n",
       "1   1774              0\n",
       "2   2395              0\n",
       "3   1765              0\n",
       "4     10              1\n",
       "5     43              1\n",
       "6   1621              0\n",
       "7   1357              0\n",
       "8   1152              0\n",
       "9   1135              0\n",
       "10   469              1\n",
       "11    23              1\n",
       "12  1478              1\n",
       "13  3693              0\n",
       "14    49              1\n",
       "15  2975              0\n",
       "16  1968              0\n",
       "17    10              1\n",
       "18  1572              0\n",
       "19  1211              0\n",
       "20   138              1\n",
       "0   1049              0\n",
       "1    626              1\n",
       "2   1450              0\n",
       "3    137              1\n",
       "4   1682              0\n",
       "5   2337              0\n",
       "6   1358              0\n",
       "7   1596              0\n",
       "8   1581              0\n",
       "..   ...            ...\n",
       "23  1822              1\n",
       "24  2051              0\n",
       "25  1941              0\n",
       "26  1952              0\n",
       "27  2257              1\n",
       "28  1340              0\n",
       "29  1444              0\n",
       "30    22              1\n",
       "31  1240              0\n",
       "32  1433              0\n",
       "33  1378              1\n",
       "34  1472              0\n",
       "35  1837              1\n",
       "36  1508              0\n",
       "37  1898              0\n",
       "38  1239              1\n",
       "39  1558              0\n",
       "40  2042              0\n",
       "41  2610              1\n",
       "42  1261              1\n",
       "43  1173              0\n",
       "44  1629              0\n",
       "45  1416              0\n",
       "46  1768              0\n",
       "47  1680              0\n",
       "48  1363              0\n",
       "49  1789              0\n",
       "50  1456              0\n",
       "51  1571              0\n",
       "52  1401              1\n",
       "\n",
       "[436 rows x 2 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positive = false_negatives.copy()\n",
    "true_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ikyk0hRmEP-"
   },
   "source": [
    "# Report of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cells are for categorizing the TP, FP, and FN objects size-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYhlBVUdmEP_",
    "outputId": "dacf29e6-1552-4265-e4ea-febbae648b4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Area\n",
       "Tiny nuclei      46\n",
       "Small nuclei     27\n",
       "Normal nuclei    35\n",
       "Large nuclei      8\n",
       "Name: False_Negative, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize False Negatives by area\n",
    "\n",
    "false_negatives = false_negatives[false_negatives[\"False_Negative\"] == 1]\n",
    "\n",
    "false_negatives.groupby(\n",
    "    pd.cut(\n",
    "        false_negatives[\"Area\"], \n",
    "        [0,100,1000,2100,30000], # Area intervals\n",
    "        labels=[\"Tiny nuclei\",\"Small nuclei\",\"Normal nuclei\",\"Large nuclei\"],\n",
    "    )\n",
    ")[\"False_Negative\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8kKlkw6mEQA",
    "outputId": "cd0c2d09-948d-4814-a787-a799bb1d7afd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area\n",
      "Tiny nuclei      15\n",
      "Small nuclei     11\n",
      "Normal nuclei     1\n",
      "Large nuclei      3\n",
      "Name: False_Positive, dtype: int64\n",
      "FDR Tiny: 0.0353\n",
      "FDR Small: 0.0259\n",
      "FDR Normal: 0.0024\n",
      "FDR Large: 0.0071\n"
     ]
    }
   ],
   "source": [
    "falseP = false_positives[false_positives[\"False_Positive\"] == 1]\n",
    "\n",
    "FP_Area_chart = falseP.groupby(\n",
    "    pd.cut(\n",
    "        falseP[\"Area\"], \n",
    "        [0,100,1000,2100,30000], # Area intervals\n",
    "        labels=[\"Tiny nuclei\",\"Small nuclei\",\"Normal nuclei\",\"Large nuclei\"],\n",
    "    )\n",
    ")[\"False_Positive\"].sum()\n",
    "\n",
    "total_Positives = len(false_positives)\n",
    "print(FP_Area_chart)\n",
    "\n",
    "print(\"FDR Tiny:\", \"%.4f\" % (FP_Area_chart[\"Tiny nuclei\"]/total_Positives))\n",
    "\n",
    "print(\"FDR Small:\", \"%.4f\" % (FP_Area_chart[\"Small nuclei\"]/total_Positives))\n",
    "\n",
    "print(\"FDR Normal:\", \"%.4f\" % (FP_Area_chart[\"Normal nuclei\"]/total_Positives))\n",
    "\n",
    "print(\"FDR Large:\", \"%.4f\" % (FP_Area_chart[\"Large nuclei\"]/total_Positives))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/berzelius-2021-21/users/Salma-files/envs/UNET_TF_1/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Area\n",
       "Tiny nuclei        0\n",
       "Small nuclei      14\n",
       "Normal nuclei    240\n",
       "Large nuclei      66\n",
       "Name: False_Negative, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize False Negatives by area\n",
    "\n",
    "true_P = true_positive[true_positive[\"False_Negative\"] == 0]\n",
    "\n",
    "true_P['False_Negative'] = 1\n",
    "TP_Area_chart = true_P.groupby(\n",
    "    pd.cut(\n",
    "        true_P[\"Area\"], \n",
    "        [0,100,1000,2100,30000], # Area intervals\n",
    "        labels=[\"Tiny nuclei\",\"Small nuclei\",\"Normal nuclei\",\"Large nuclei\"],\n",
    "    )\n",
    ")[\"False_Negative\"].sum()\n",
    "TP_Area_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Area\n",
       "Tiny nuclei       46\n",
       "Small nuclei      41\n",
       "Normal nuclei    275\n",
       "Large nuclei      74\n",
       "Name: Gold_Obj, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_obj['Gold_Obj'] = 1\n",
    "\n",
    "\n",
    "GO_Area_chart = gold_obj.groupby(\n",
    "    pd.cut(\n",
    "        gold_obj[\"Area\"], \n",
    "        [0,100,1000,2100,30000], # Area intervals\n",
    "        labels=[\"Tiny nuclei\",\"Small nuclei\",\"Normal nuclei\",\"Large nuclei\"],\n",
    "    )\n",
    ")['Gold_Obj'].sum()\n",
    "\n",
    "\n",
    "print(GO_Area_chart.sum())\n",
    "\n",
    "GO_Area_chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTVkZshjmEQA",
    "outputId": "ceffd704-a488-4519-e3d0-45ce1d2f6b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: 5\n",
      "Merges: 5\n"
     ]
    }
   ],
   "source": [
    "# Summarize splits and merges\n",
    "\n",
    "print(\"Splits:\",np.sum(splits_merges[\"Splits\"]))\n",
    "print(\"Merges:\",np.sum(splits_merges[\"Merges\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vb5A0H9-mEQA",
    "outputId": "a4be33ef-8631-47ec-9c66-eace00c919bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra objects (false postives): 24\n"
     ]
    }
   ],
   "source": [
    "# Report false positives\n",
    "\n",
    "print(\"Extra objects (false postives):\",results[results[\"Threshold\"].round(3) == 0.9].sum()[\"FP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_IA6IZImEQB",
    "outputId": "ce2577c3-91ed-43db-f865-435963ac27ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra objects (false postives): 30\n",
      "True positives: 320\n",
      "Total objects: 350\n",
      "False discovery rate: 0.08571428571428572\n"
     ]
    }
   ],
   "source": [
    "thresh = 0.9\n",
    "\n",
    "\n",
    "FP = results[results[\"Threshold\"].round(3) == thresh].sum()[\"FP\"]\n",
    "FN = results[results[\"Threshold\"].round(3) == thresh].sum()[\"FN\"]\n",
    "TP = results[results[\"Threshold\"].round(3) == thresh].sum()[\"TP\"]\n",
    "Total = FP + TP\n",
    "\n",
    "print(\"Extra objects (false postives):\",FP)\n",
    "\n",
    "print(\"True positives:\",TP)\n",
    "\n",
    "print(\"Total objects:\",Total)\n",
    "\n",
    "print(\"False discovery rate:\", FP/Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIwS7lEk4AIf"
   },
   "source": [
    "# **EPOCHS COMPARISON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2liS9NV4Dmq",
    "outputId": "e02233a2-1c27-4546-b061-61072acf7174",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################\n",
      "Model number 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/berzelius-2021-21/users/Salma-files/envs/UNET_TF_1/lib/python3.6/site-packages/ipykernel_launcher.py:101: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/proj/berzelius-2021-21/users/Salma-files/envs/UNET_TF_1/lib/python3.6/site-packages/ipykernel_launcher.py:130: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Threshold        F1   Jaccard\n",
      "0       0.50  0.939906  0.871175\n",
      "1       0.55  0.932127  0.871175\n",
      "2       0.60  0.927704  0.871175\n",
      "3       0.65  0.924396  0.871175\n",
      "4       0.70  0.915434  0.871175\n",
      "5       0.75  0.908220  0.871175\n",
      "6       0.80  0.898602  0.871175\n",
      "7       0.85  0.873029  0.871175\n",
      "8       0.90  0.809952  0.871175\n",
      "9       0.95  0.264892  0.871175\n",
      "Average F1 score: 0.8099523685445901\n",
      "Jaccard index: 0.8711750440345224\n",
      "False negatives: 89\n",
      "False postives: 17\n",
      "True positives: 240\n",
      "Total objects: 257\n",
      "False discovery rate: 0.06614785992217899\n",
      "#############################################\n",
      "################################################################\n",
      "Model number 12\n",
      "   Threshold        F1   Jaccard\n",
      "0       0.50  0.964237  0.880613\n",
      "1       0.55  0.955036  0.880613\n",
      "2       0.60  0.950155  0.880613\n",
      "3       0.65  0.942650  0.880613\n",
      "4       0.70  0.935616  0.880613\n",
      "5       0.75  0.926865  0.880613\n",
      "6       0.80  0.919195  0.880613\n",
      "7       0.85  0.898094  0.880613\n",
      "8       0.90  0.806144  0.880613\n",
      "9       0.95  0.312158  0.880613\n",
      "Average F1 score: 0.8061442532709678\n",
      "Jaccard index: 0.880612726844587\n",
      "False negatives: 98\n",
      "False postives: 9\n",
      "True positives: 231\n",
      "Total objects: 240\n",
      "False discovery rate: 0.0375\n",
      "#############################################\n",
      "################################################################\n",
      "Model number 14\n",
      "   Threshold        F1  Jaccard\n",
      "0       0.50  0.958631  0.87835\n",
      "1       0.55  0.951714  0.87835\n",
      "2       0.60  0.948936  0.87835\n",
      "3       0.65  0.945752  0.87835\n",
      "4       0.70  0.937416  0.87835\n",
      "5       0.75  0.927719  0.87835\n",
      "6       0.80  0.917959  0.87835\n",
      "7       0.85  0.889055  0.87835\n",
      "8       0.90  0.846058  0.87835\n",
      "9       0.95  0.384441  0.87835\n",
      "Average F1 score: 0.8460575227731557\n",
      "Jaccard index: 0.8783503662104977\n",
      "False negatives: 77\n",
      "False postives: 17\n",
      "True positives: 252\n",
      "Total objects: 269\n",
      "False discovery rate: 0.06319702602230483\n",
      "#############################################\n"
     ]
    }
   ],
   "source": [
    "#all_images = data_partitions[partition]\n",
    "\n",
    "\n",
    "config_vars[\"raw_annotations_dir\"] = '../data/1_raw_annotations/'\n",
    "config_vars[\"labels_out_dir\"]      =\\\n",
    "'/proj/berzelius-2021-21/users/klara/Segmentation/data/experiments/Model_3_Malou/out_3/segm/'\n",
    "all_images = data_partitions['validation']\n",
    "config_vars[\"object_dilation\"] =3\n",
    "\n",
    "from skimage.color import rgb2gray,rgb2lab\n",
    "\n",
    "epochs_results = pd.DataFrame(columns=[\"Average F1 Score\", \"Average Jaccard Index\", \"False Negatives\", \"False Positives\", \"True Positives\", \"Detected Objects\", \"False Discovery Rate\"])\n",
    "best_models = [3,12,14]\n",
    "for m_n in best_models:\n",
    "    config_vars[\"labels_out_dir\"]      = '../data/experiments/Model_3_Malou/out_'+str(m_n)+'/segm/'\n",
    "\n",
    "    print(\"################################################################\")\n",
    "    print(\"Model number\", m_n)\n",
    "\n",
    "    results = pd.DataFrame(columns=[\"Image\", \"Threshold\", \"F1\", \"Jaccard\", \"TP\", \"FP\", \"FN\"])\n",
    "    false_negatives = pd.DataFrame(columns=[\"False_Negative\", \"Area\"])\n",
    "    false_positives = pd.DataFrame(columns=[\"False_Positive\", \"Area\"])\n",
    "    gold_obj = pd.DataFrame(columns=[\"Gold_Object\", \"Area\"])\n",
    "    pred_obj = pd.DataFrame(columns=[\"Pred_Object\", \"Area\"])\n",
    "    splits_merges = pd.DataFrame(columns=[\"Image_Name\", \"Merges\", \"Splits\"])\n",
    "\n",
    "    for image_name in all_images:\n",
    "      # Load ground truth data  \n",
    "        img_filename = os.path.join(config_vars[\"raw_annotations_dir\"], image_name)\n",
    "        ground_truth = skimage.io.imread(img_filename)\n",
    "        #ground_truth = ground_truth.squeeze()\n",
    "        if len(ground_truth.shape) == 3:\n",
    "            ground_truth = rgb2lab(ground_truth)\n",
    "            ground_truth = ground_truth[:,:,0]\n",
    "\n",
    "        ground_truth = skimage.morphology.label(ground_truth)\n",
    "\n",
    "        # Transform to label matrix\n",
    "        #ground_truth = skimage.morphology.label(ground_truth)\n",
    "\n",
    "        # Load predictions\n",
    "        pred_filename = os.path.join(config_vars[\"labels_out_dir\"], image_name)\n",
    "        prediction = skimage.io.imread(pred_filename)\n",
    "\n",
    "\n",
    "        if len(prediction.shape) == 3:\n",
    "            prediction = rgb2lab(prediction)\n",
    "            prediction = prediction[:,:,0]\n",
    "\n",
    "\n",
    "        gold_obj = get_gold_objects(\n",
    "              ground_truth, \n",
    "              prediction, \n",
    "              gold_obj, \n",
    "              image_name\n",
    "          )\n",
    "        pred_obj = get_predicted_objects(\n",
    "              ground_truth, \n",
    "              prediction, \n",
    "              pred_obj, \n",
    "              image_name\n",
    "          )\n",
    "\n",
    "          # Apply object dilation\n",
    "        if config_vars[\"object_dilation\"] > 0:\n",
    "            struct = skimage.morphology.square(config_vars[\"object_dilation\"])\n",
    "            prediction = skimage.morphology.dilation(prediction, struct)\n",
    "        elif config_vars[\"object_dilation\"] < 0:\n",
    "            struct = skimage.morphology.square(-config_vars[\"object_dilation\"])\n",
    "            prediction = skimage.morphology.erosion(prediction, struct)\n",
    "\n",
    "        ####################################################################################    \n",
    "        #### Testing prediction with no small objects on annot and prediction #####\n",
    "        ground_truth = skimage.morphology.remove_small_objects(ground_truth, min_size=100) \n",
    "        prediction = skimage.morphology.remove_small_objects(prediction, min_size=100)\n",
    "        #####################################################################################\n",
    "\n",
    "        # Relabel objects (cut margin of 30 pixels to make a fair comparison with DeepCell)\n",
    "        ground_truth = skimage.segmentation.relabel_sequential(ground_truth)[0] #[30:-30,30:-30])[0]\n",
    "        prediction = skimage.segmentation.relabel_sequential(prediction)[0] #[30:-30,30:-30])[0]\n",
    "\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        results = compute_af1_results(\n",
    "          ground_truth, \n",
    "          prediction, \n",
    "          results, \n",
    "          image_name\n",
    "        )\n",
    "\n",
    "        false_negatives = get_false_negatives(\n",
    "          ground_truth, \n",
    "          prediction, \n",
    "          false_negatives, \n",
    "          image_name\n",
    "        )\n",
    "\n",
    "        false_positives = get_false_positives(\n",
    "          ground_truth, \n",
    "          prediction, \n",
    "          false_positives, \n",
    "          image_name\n",
    "        )\n",
    "\n",
    "        splits_merges = get_splits_and_merges(\n",
    "          ground_truth, \n",
    "          prediction, \n",
    "          splits_merges, \n",
    "          image_name\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # Display an example image\n",
    "  #if image_name == all_images[0]:\n",
    "      #show(ground_truth, prediction, image_name=image_name)\n",
    "\n",
    "\n",
    "    # Display accuracy results\n",
    "\n",
    "    average_performance = results.groupby(\"Threshold\").mean().reset_index()\n",
    "\n",
    "\n",
    "    R = results.groupby(\"Image\").mean().reset_index()\n",
    "    #g = sb.jointplot(data=R[R[\"F1\"] > 0.4], x=\"Jaccard\", y=\"F1\")\n",
    "\n",
    "    #average_performance\n",
    "    R.sort_values(by=\"F1\",ascending=False)\n",
    "\n",
    "    # Plot accuracy results\n",
    "\n",
    "    #sb.regplot(data=average_performance, x=\"Threshold\", y=\"F1\", order=3, ci=None)\n",
    "    print(average_performance)\n",
    "\n",
    "\n",
    "    # Compute and print Average F1\n",
    "\n",
    "    F1_score_90 = average_performance[\"F1\"][8] # I want only 90%\n",
    "\n",
    "    #average_performance[\"F1\"].mean()\n",
    "    jaccard_index = average_performance[\"Jaccard\"].mean()\n",
    "    print(\"Average F1 score:\", F1_score_90 )\n",
    "    print(\"Jaccard index:\", jaccard_index)\n",
    "\n",
    "\n",
    "    # calculate true postives and false positives\n",
    "    thresh = 0.9\n",
    "\n",
    "    FP = results[results[\"Threshold\"].round(3) == thresh].sum()[\"FP\"]\n",
    "    TP = results[results[\"Threshold\"].round(3) == thresh].sum()[\"TP\"]\n",
    "    FN = results[results[\"Threshold\"].round(3) == thresh].sum()[\"FN\"]\n",
    "    Total = FP + TP\n",
    "\n",
    "    print(\"False negatives:\",FN)\n",
    "    print(\"False postives:\",FP)\n",
    "    print(\"True positives:\",TP)\n",
    "\n",
    "    print(\"Total objects:\",Total)\n",
    "\n",
    "    print(\"False discovery rate:\", FP/Total)\n",
    "\n",
    "    epochs_results.loc[m_n] = [F1_score_90 , jaccard_index, FN, FP, TP, Total, FP/Total]\n",
    "\n",
    "    print('#############################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_score_90</th>\n",
       "      <th>Average Jaccard Index</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>True Positives</th>\n",
       "      <th>Detected Objects</th>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.809952</td>\n",
       "      <td>0.871175</td>\n",
       "      <td>89.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>0.933852</td>\n",
       "      <td>0.729483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.806144</td>\n",
       "      <td>0.880613</td>\n",
       "      <td>98.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.702128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.846058</td>\n",
       "      <td>0.878350</td>\n",
       "      <td>77.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.063197</td>\n",
       "      <td>0.936803</td>\n",
       "      <td>0.765957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    F1_score_90  Average Jaccard Index  False Negatives  False Positives  \\\n",
       "3      0.809952               0.871175             89.0             17.0   \n",
       "12     0.806144               0.880613             98.0              9.0   \n",
       "14     0.846058               0.878350             77.0             17.0   \n",
       "\n",
       "    True Positives  Detected Objects  False Discovery Rate  Precision  \\\n",
       "3            240.0             257.0              0.066148   0.933852   \n",
       "12           231.0             240.0              0.037500   0.962500   \n",
       "14           252.0             269.0              0.063197   0.936803   \n",
       "\n",
       "      Recall  \n",
       "3   0.729483  \n",
       "12  0.702128  \n",
       "14  0.765957  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs_results.rename(columns={'Average F1 Score':'F1_score_90'},inplace=True)\n",
    "epochs_results['Precision']=epochs_results['True Positives']/(epochs_results['False Positives']+epochs_results['True Positives'])\n",
    "epochs_results['Recall']=epochs_results['True Positives']/(epochs_results['False Negatives']+epochs_results['True Positives'])\n",
    "\n",
    "epochs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eIx6kuck7oo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KzVPXA30kvwk",
    "XRyIwrZqmEP3",
    "F-DIQl9FmEP7",
    "3k0420T2fKzn"
   ],
   "name": "HoverNet_evaluation_nuclei.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
